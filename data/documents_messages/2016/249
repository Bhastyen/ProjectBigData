listeegc iso88591qthe8se cifre bonjour trouverez dessous the8se apprentissage renforcement propose9 orange labs contrat cifre contact orange  raphaebl fe9raud  raphael feraud orange com mailto raphael feraud orange com   orange labs lannionencadrement acade9mique   odalric maillard  odalric maillard inria fr mailto odalric maillard inria fr   lri orsayobjectif the8sel objectif cette the8se de9velopper analyser algorithmes bandit contextuel e0 me9moire optimisation choix actions environnement dynamique automatisation prolife9ration algorithmes de9cisions services re9seaux te9le9communication ne9cessitent contrf4ler cofbt potentiel mauvaises de9cisions  ne9cessaire utiliser algorithmes disposants fortes garanties the9oriques algorithmes e9tudie9s lors the8se seront teste9s simulation cas usages optimisation marketing  routage allocation ressources re9seau te9le9communication  issue the8se  objectif inte9grer algorithmes utilisation ope9rationnelle etat arten interagissant flux e9ve8nements  algorithmes machine learning optimisent choix publicite9s  choisissent meilleure interface homme machine  recommandent produits  assurent premier niveau service apre8s vente  se9lectionnent meilleur re9seau fil mobile  optimisent allocation ressources re9seau te9le9communication    applications  choisir action contexte observable  exemple profil client   retour environnement partiel   seule re9compense action choisie connue   se9quence re9compenses ge9ne9re9e syste8me dynamique inconnu  changements distribution re9compenses contextes peuvent intervenir  proble8me bandits manchots consiste e0 optimiser se9quentiellement choix action lorsque environnement ne fournit qu retour partiel  algorithmes e9value9s termes regret entre re9compenses obtenues actions choisies celles auraient e9te9 obtenues choisissant meilleure action  solutions optimales termes regret efficaces termes temps traitement ont e9te9 propose9es  1  prendre de9cisions algorithme dispose souvent contexte  cas optimisation campagnes marketing  contexte profil client action choix campagne marketing  campagne marketing client   proble8me bandits contextuels consiste e0 construire mode8le se9quentiellement explorer exploiter ensemble actions fonction contextes observe9s  proble8me bandit contextuel eatre re9duit e0 se9rie proble8mes bandits manchots utilisant mode8les hie9rarchiques arbres de9cisions foreats ale9atoires  2  applications  se9quences contextes re9compenses ne peuvent eatre conside9re9es e9tant identiquement inde9pendamment distribue9es  exemple  lors optimisation campagnes marketing  nouvelle offre chez concurrent changer significativement re9action clients face e0 plusieurs campagnes marketing  ge9rer changements  premie8re approche consiste e0 faire hypothe8se se9quences contextes re9compenses ont e9te9 ge9ne9re9es e0 avance adversaire  solutions optimales proble8me bandits adverses ont e9te9 propose9es  3   ne9anmoins  solutions efficaces pratique conservatrices changements environnement parfois bien connus  comportement achat pendant pe9riode feates diffe9rent celui reste anne9e  ide9e naturelle utiliser processus de9cision markovien mode9liser  cas ge9ne9ral  structure e9tats connue  processus de9cision markovien partiellement observables peuvent mode9liser incertitude mode8le e0 e9tats  ne9anmoins  solutions optimales incalculables  4   cadre apprentissage renforcement nombreuses solutions approche9es ont e9te9 propose9es  5  se basant learning  mode8les markov cache9s  re9seaux neurones re9currents   re9cemment  de9passer limites re9seaux neurones re9currents notamment difficulte9 apprendre me9moire e0 long terme  plusieurs auteurs ont propose9 utiliser me9moire explicite  6   cette ide9e parait prometteuse e9tudier bandits contextuels environnement dynamique prendre de9cision  joueur se base me9moire   e9tat syste8me  e9ve8nement passe9  observation contexte  re9compense permet mettre e0 jour politique joueur me9moire approche me9thodologique planningpour re9aliser travail recherche  doctorant devra maitriser algorithmes mode8les apprentissage renforcement  notamment bandits manchots  bandits contextuels  processus de9cision markoviens  travail approfondi bibliographie crucial  quelques tests devront eatre mene9s paralle8le principaux algorithmes propose9es e9tat art afin bien comprendre leurs avantages inconve9nients  doctorant pourra ensuite aborder principale difficulte9 analyse bandits e0 me9moire e0 savoir classe processus de9cision markovien partiellement observables partie classe proble8mes pspace hard  qu existe algorithme efficace re9soudre proble8me cas ge9ne9ral  doctorant devra e9tablir hypothe8ses raisonnables mode8le repre9sentation e9tats syste8me  afin e9laborer algorithmes efficaces disposant forte garantie the9orique  pourra notamment inspirer travaux  7   8  deuxie8me difficulte9  devra aborder doctorant  lie9e e9valuation empirique algorithme interagissant environnement  ne fournit re9compense actions choisies  proble8me soi  de9ployer algorithme environnement ope9rationnel e0 fins e9valuation e9tant option possible  sera ne9cessaire recourir e0 simulations  premier temps seront produites donne9es synthe9tiques e0 partir jeux donne9es re9fe9rence afin se comparer e9tat art  tests seront ensuite comple9te9s simulations donne9es logs provenant orange afin approcher mieux cas usages re9els re9fe9rences  1  auer    cesa bianchi    fischer    finite time analysis of the multiarmed bandit problem  machine learning 47  235 256  2002  2  fe9raud    allesiardo    urvoy    cle9rot    random forest for the contextual bandit problem  aistats 2016  3  auer    cesa bianchi    freund    schapire     the nonstochastic multiarmed bandit problem  siam  comput   32 48 77  2002  4  papadimitriou     tsitsiklis       the complexity of markov decision processes  mathematics of operations research  1987  5  aberdeen     revised  survey of approximate methods for solving partially observable markov decision processes technical report  research school of information science and engineering  australia national university  2003  6  graves    wayne    danihelka    neural turing machines  http   arxiv org abs 1410 5401  2015  7  maillard     munos    ryabko    selecting the state representation in reinforcement learning  nips  2012  8  hamilton    milani fard    pineau    efficient learning and planning with compressed predictive states  jmlr  2014 _________________________________________________________________________________________________________________________ce message pieces jointes peuvent contenir informations confidentielles privilegiees ne doivent doncpas etre diffuses  exploites copies autorisation  avez recu message erreur  veuillez signalera expediteur detruire ainsi pieces jointes  messages electroniques etant susceptibles alteration orange decline toute responsabilite message ete altere  deforme falsifie  merci this message and its attachments may contain confidential or privileged information that may be protected by law they should not be distributed  used or copied without authorisation if you have received this email in error  please notify the sender and delete this message and its attachments as emails may be altered  orange is not liable for messages that have been modified  changed or falsified thank you 
Return-Path: <raphael.feraud@orange.com>
X-Original-To: polytech_liste-egc@sympa6.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa6.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (BounceSMTP2.univ-nantes.prive [172.20.12.67])
	by sympa6.univ-nantes.prive (Postfix) with ESMTP id D978E1903983
	for <polytech_liste-egc@sympa6.univ-nantes.prive>; Thu, 31 Mar 2016 18:08:53 +0200 (CEST)
Received: from mx2.d101.univ-nantes.fr (MX2.univ-nantes.fr [193.52.101.136])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id D544960FB3C
	for <polytech_liste-egc@sympa6.univ-nantes.prive>; Thu, 31 Mar 2016 18:08:53 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by mx2.d101.univ-nantes.fr (Postfix) with ESMTP id CB8C6B4B130
	for <liste-egc@polytech.univ-nantes.fr>; Thu, 31 Mar 2016 18:08:53 +0200 (CEST)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: -3.415
X-Spam-Level:
X-Spam-Status: No, score=-3.415 tagged_above=-1000 required=5
	tests=[CRM114_GOOD=-5, DNS_FROM_AHBL_RHSBL=0.01, FREEMAIL_FROM=0.001,
	HTML_MESSAGE=0.001, NO_RDNS=0.5, NO_REAL_NAME=1, RCVD_IN_WSFF=0.01,
	RP_MATCHES_RCVD=-0.147, SPF_PASS=-0.001, T_FRT_ADULT2=0.01,
	UNPARSEABLE_RELAY=0.001, UN_PHISHING_PW=0.1, UN_PHISHING_WEBMAIL=0.1]
	autolearn=disabled
X-CRM114-Status: GOOD ( 10.4275 )
X-CRM114-CacheID: 
Received: from mx2.d101.univ-nantes.fr ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id F4X3stkzIxmZ for <liste-egc@polytech.univ-nantes.fr>;
	Thu, 31 Mar 2016 18:08:49 +0200 (CEST)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from relais-inet.orange.com (relais-nor34.orange.com [80.12.70.34])
	by mx2.d101.univ-nantes.fr (Postfix) with ESMTPS id 6D7A9624237
	for <liste-egc@polytech.univ-nantes.fr>; Thu, 31 Mar 2016 18:08:49 +0200 (CEST)
Received: from opfednr07.francetelecom.fr (unknown [xx.xx.xx.71])
	by opfednr21.francetelecom.fr (ESMTP service) with ESMTP id DE5D9C01A8
	for <liste-egc@polytech.univ-nantes.fr>; Thu, 31 Mar 2016 18:08:48 +0200 (CEST)
Received: from Exchangemail-eme2.itn.ftgroup (unknown [xx.xx.31.72])
	by opfednr07.francetelecom.fr (ESMTP service) with ESMTP id B55111C005D
	for <liste-egc@polytech.univ-nantes.fr>; Thu, 31 Mar 2016 18:08:48 +0200 (CEST)
Received: from OPEXCLILM23.corporate.adroot.infra.ftgroup
 ([fe80::787e:db0c:23c4:71b3]) by OPEXCLILMA3.corporate.adroot.infra.ftgroup
 ([fe80::60a9:abc3:86e6:2541%19]) with mapi id 14.03.0279.002; Thu, 31 Mar
 2016 18:08:48 +0200
From: <raphael.feraud@orange.com>
To: "liste-egc@polytech.univ-nantes.fr" <liste-egc@polytech.univ-nantes.fr>
Thread-Topic: =?iso-8859-1?Q?Th=E8se_Cifre_=E0_Orange_Labs:_Bandits_=E0_M=E9moire_pour_?=
 =?iso-8859-1?Q?la_prise_de_d=E9cision_en_environnement_dynamique?=
Thread-Index: AdGLZELzAd/pMV7eTEC2aoVyk13aEg==
Date: Thu, 31 Mar 2016 16:08:47 +0000
Message-ID: <23464_1459440528_56FD4B90_23464_9490_1_CFC3D1D3F172164888E30D094F2A10920FB94FDF@OPEXCLILM23.corporate.adroot.infra.ftgroup>
Accept-Language: fr-FR, en-US
Content-Language: fr-FR
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.168.234.1]
Content-Type: multipart/alternative;
	boundary="_000_CFC3D1D3F172164888E30D094F2A10920FB94FDFOPEXCLILM23corp_"
MIME-Version: 1.0
X-Validation-by: cyril.de-runz@univ-reims.fr
Subject: [liste-egc] =?ISO-8859-1?Q?Th=E8se?= Cifre =?ISO-8859-1?Q?=E0?=
 Orange Labs: Bandits =?ISO-8859-1?Q?=E0_M=E9moire?= pour la prise de
 =?ISO-8859-1?Q?d=E9cision?= en environnement dynamique


--_000_CFC3D1D3F172164888E30D094F2A10920FB94FDFOPEXCLILM23corp_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Bonjour,

Vous trouverez ci-dessous un sujet de th=E8se en apprentissage par renforce=
ment propos=E9 par Orange Labs sous contrat Cifre.
Contact orange: Rapha=EBl F=E9raud, raphael.feraud@orange.com<mailto:raphae=
l.feraud@orange.com>, Orange Labs Lannion
Encadrement acad=E9mique : Odalric Maillard, odalric.maillard@inria.fr<mail=
to:odalric.maillard@inria.fr>, LRI Orsay

Objectif de la th=E8se

L'objectif de cette th=E8se est de d=E9velopper et d'analyser des algorithm=
es de bandit contextuel =E0 m=E9moire pour l'optimisation du choix d'action=
s dans un environnement dynamique.

L'automatisation et la prolif=E9ration des algorithmes de d=E9cisions dans =
les services et r=E9seaux de t=E9l=E9communication n=E9cessitent de contr=
=F4ler le co=FBt potentiel des mauvaises d=E9cisions. C'est pourquoi il est=
 n=E9cessaire d'utiliser des algorithmes disposants de fortes garanties th=
=E9oriques.

Les algorithmes =E9tudi=E9s lors de la th=E8se seront test=E9s en simulatio=
n sur des cas d'usages d'optimisation marketing, de routage et d'allocation=
 de ressources dans un r=E9seau de t=E9l=E9communication. A l'issue de la t=
h=E8se, l'objectif est d'int=E9grer les algorithmes pour une utilisation op=
=E9rationnelle.

Etat de l'art

En interagissant avec des flux d'=E9v=E8nements, des algorithmes de machine=
 learning optimisent le choix des publicit=E9s, choisissent la meilleure in=
terface homme machine, recommandent des produits, assurent un premier nivea=
u de service apr=E8s-vente, s=E9lectionnent le meilleur r=E9seau sans fil p=
our chaque mobile, optimisent l'allocation de ressources dans un r=E9seau d=
e t=E9l=E9communication... Dans la plupart de ces applications, pour choisi=
r l'action un contexte est observable (par exemple le profil du client), le=
 retour de l'environnement est partiel (i.e. seule la r=E9compense de l'act=
ion choisie est connue), et la s=E9quence de r=E9compenses est g=E9n=E9r=E9=
e par un syst=E8me dynamique inconnu (des changements de la distribution de=
s r=E9compenses et des contextes peuvent intervenir).
Le probl=E8me des bandits manchots consiste =E0 optimiser s=E9quentiellemen=
t le choix d'une action lorsque l'environnement ne fournit qu'un retour par=
tiel. Les algorithmes sont =E9valu=E9s en termes de regret entre les r=E9co=
mpenses obtenues par les actions choisies et celles qui auraient =E9t=E9 ob=
tenues en choisissant la meilleure action. Des solutions optimales en terme=
s de regret et efficaces en termes de temps de traitement ont =E9t=E9 propo=
s=E9es [1].
Pour prendre ses d=E9cisions l'algorithme dispose souvent d'un contexte. Da=
ns le cas de l'optimisation de campagnes marketing, le contexte est le prof=
il d'un client et l'action est le choix de la campagne marketing: quelle ca=
mpagne marketing pour quel client ? Le probl=E8me des bandits contextuels c=
onsiste =E0 construire un mod=E8le s=E9quentiellement pour explorer et expl=
oiter un ensemble d'actions en fonction des contextes observ=E9s. Le probl=
=E8me du bandit contextuel peut =EAtre r=E9duit =E0 une s=E9rie de probl=E8=
mes de bandits manchots en utilisant des mod=E8les hi=E9rarchiques tels que=
 les arbres de d=E9cisions ou les for=EAts al=E9atoires [2].
Dans la plupart des applications, les s=E9quences de contextes et de r=E9co=
mpenses ne peuvent pas =EAtre consid=E9r=E9es comme =E9tant identiquement e=
t ind=E9pendamment distribu=E9es. Par exemple, lors de l'optimisation de ca=
mpagnes marketing, une nouvelle offre chez un concurrent peut changer signi=
ficativement la r=E9action de tous les clients face =E0 une ou plusieurs de=
s campagnes marketing. Pour g=E9rer ces changements, une premi=E8re approch=
e consiste =E0 faire l'hypoth=E8se que les s=E9quences de contextes et de r=
=E9compenses ont =E9t=E9 g=E9n=E9r=E9es =E0 l'avance par un adversaire. Des=
 solutions optimales pour le probl=E8me des bandits adverses ont =E9t=E9 pr=
opos=E9es [3]. N=E9anmoins, ces solutions sont peu efficaces en pratique ca=
r trop conservatrices.
Les changements de l'environnement sont parfois bien connus: le comportemen=
t d'achat pendant la p=E9riode des f=EAtes est diff=E9rent de celui du rest=
e de l'ann=E9e. L'id=E9e naturelle est alors d'utiliser un Processus de D=
=E9cision Markovien pour les mod=E9liser. Dans le cas g=E9n=E9ral, la struc=
ture des =E9tats n'est pas connue. Les Processus de D=E9cision Markovien Pa=
rtiellement Observables peuvent mod=E9liser l'incertitude sur le mod=E8le =
=E0 =E9tats. N=E9anmoins, les solutions optimales sont incalculables [4]. D=
ans le cadre de l'Apprentissage par Renforcement de nombreuses solutions ap=
proch=E9es ont =E9t=E9 propos=E9es [5] se basant sur le Q-learning, sur les=
 Mod=E8les de Markov Cach=E9s, les R=E9seaux de Neurones R=E9currents...
R=E9cemment, pour d=E9passer les limites des R=E9seaux de Neurones R=E9curr=
ents et notamment la difficult=E9 d'apprendre une m=E9moire =E0 long terme,=
 plusieurs auteurs ont propos=E9 d'utiliser une m=E9moire explicite [6]. Ce=
tte id=E9e nous parait prometteuse pour =E9tudier les bandits contextuels d=
ans un environnement dynamique:
Pour prendre une d=E9cision, le joueur se base sur sa m=E9moire (i.e. un =
=E9tat du syst=E8me, ou un =E9v=E8nement pass=E9) et sur l'observation du c=
ontexte. La r=E9compense permet de mettre =E0 jour la politique du joueur e=
t sa m=E9moire.

Approche m=E9thodologique-planning

Pour r=E9aliser ce travail de recherche, le doctorant devra maitriser les a=
lgorithmes et mod=E8les d'apprentissage par renforcement, et notamment les =
bandits manchots, les bandits contextuels, les Processus de D=E9cision Mark=
oviens. Un travail approfondi de bibliographie est crucial. Quelques tests =
devront =EAtre men=E9s en parall=E8le sur les principaux algorithmes propos=
=E9es par l'=E9tat de l'art afin de bien comprendre leurs avantages et inco=
nv=E9nients. Le doctorant pourra ensuite aborder la principale difficult=E9=
 de l'analyse des bandits =E0 m=E9moire =E0 savoir le fait que la classe de=
s Processus de D=E9cision Markovien Partiellement Observables fait partie d=
e la classe des probl=E8mes PSPACE-Hard, et qu'il n'existe donc pas d'algor=
ithme efficace pour r=E9soudre ce probl=E8me dans le cas g=E9n=E9ral. Le do=
ctorant devra =E9tablir des hypoth=E8ses raisonnables sur le mod=E8le de re=
pr=E9sentation des =E9tats du syst=E8me, afin d'=E9laborer des algorithmes =
efficaces et disposant de forte garantie th=E9orique. Il pourra notamment s=
'inspirer des travaux de [7] et [8].

La deuxi=E8me difficult=E9, que devra aborder le doctorant, est li=E9e au f=
ait que l'=E9valuation empirique d'un algorithme interagissant avec un envi=
ronnement, qui ne fournit que la r=E9compense des actions choisies, est un =
probl=E8me en soi. D=E9ployer un algorithme dans un environnement op=E9rati=
onnel =E0 des fins d'=E9valuation n'=E9tant pas une option possible, il ser=
a n=E9cessaire de recourir =E0 des simulations. Dans un premier temps elles=
 seront produites avec des donn=E9es synth=E9tiques ou =E0 partir de jeux d=
e donn=E9es de r=E9f=E9rence afin de se comparer avec l'=E9tat de l'art. Ce=
s tests seront ensuite compl=E9t=E9s avec des simulations sur des donn=E9es=
 de logs provenant du SI d'Orange afin de s'approcher au mieux des cas d'us=
ages r=E9els.


R=E9f=E9rences:

[1] Auer, P., Cesa Bianchi, N., Fischer, P.: Finite-time Analysis of the Mu=
ltiarmed Bandit Problem, Machine Learning,47, 235-256, 2002.

[2] F=E9raud, R., Allesiardo, R., Urvoy, T., Cl=E9rot, F.: Random Forest fo=
r the Contextual Bandit Problem, AISTATS 2016.

[3] Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R. E.: The nonstochas=
tic multiarmed bandit problem, SIAM J. COMPUT., 32 48-77, 2002.

[4] Papadimitriou, C. H., Tsitsiklis, J., N. : The Complexity of Markov Dec=
ision Processes, Mathematics of Operations Research, 1987.

[5] Aberdeen, D.: A (revised) survey of approximate methods for solving par=
tially observable Markov decision processes,Technical report, Research Scho=
ol of Information Science and Engineering, Australia National University, 2=
003.

[6] Graves, A., Wayne, G., Danihelka, I.: Neural Turing Machines, http://ar=
xiv.org/abs/1410.5401, 2015.

[7] Maillard, O. A., Munos, R., Ryabko, D.: Selecting the State-Representat=
ion in Reinforcement Learning, NIPS, 2012.

[8] Hamilton, W., Milani Fard, M., Pineau, J.: Efficient Learning and Plann=
ing with Compressed Predictive States, JMLR, 2014.


___________________________________________________________________________=
______________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confiden=
tielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu=
 ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages el=
ectroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou =
falsifie. Merci.

This message and its attachments may contain confidential or privileged inf=
ormation that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and dele=
te this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been =
modified, changed or falsified.
Thank you.


--_000_CFC3D1D3F172164888E30D094F2A10920FB94FDFOPEXCLILM23corp_
Content-Type: text/html; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

<html xmlns:v=3D"urn:schemas-microsoft-com:vml" xmlns:o=3D"urn:schemas-micr=
osoft-com:office:office" xmlns:w=3D"urn:schemas-microsoft-com:office:word" =
xmlns:m=3D"http://schemas.microsoft.com/office/2004/12/omml" xmlns=3D"http:=
//www.w3.org/TR/REC-html40">
<head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Diso-8859-=
1">
<meta name=3D"Generator" content=3D"Microsoft Word 14 (filtered medium)">
<style><!--
/* Font Definitions */
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0cm;
	margin-bottom:.0001pt;
	font-size:11.0pt;
	font-family:"Calibri","sans-serif";
	mso-fareast-language:EN-US;}
p.MsoCommentText, li.MsoCommentText, div.MsoCommentText
	{mso-style-link:"Commentaire Car";
	margin:0cm;
	margin-bottom:.0001pt;
	font-size:10.0pt;
	font-family:"Arial","sans-serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{mso-style-type:personal-compose;
	font-family:"Calibri","sans-serif";
	color:windowtext;}
span.CommentaireCar
	{mso-style-name:"Commentaire Car";
	mso-style-link:Commentaire;
	font-family:"Arial","sans-serif";
	mso-fareast-language:FR;}
.MsoChpDefault
	{mso-style-type:export-only;
	mso-fareast-language:EN-US;}
@page WordSection1
	{size:612.0pt 792.0pt;
	margin:70.85pt 70.85pt 70.85pt 70.85pt;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext=3D"edit" spidmax=3D"1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext=3D"edit">
<o:idmap v:ext=3D"edit" data=3D"1" />
</o:shapelayout></xml><![endif]-->
</head>
<body lang=3D"FR" link=3D"blue" vlink=3D"purple">
<div class=3D"WordSection1">
<p class=3D"MsoNormal"><span lang=3D"EN-US">Bonjour,<o:p></o:p></span></p>
<p class=3D"MsoNormal"><span lang=3D"EN-US"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal">Vous trouverez ci-dessous un sujet de th=E8se en app=
rentissage par renforcement propos=E9 par Orange Labs sous contrat Cifre.<o=
:p></o:p></p>
<p class=3D"MsoNormal">Contact&nbsp;orange: Rapha=EBl F=E9raud, <a href=3D"=
mailto:raphael.feraud@orange.com">
raphael.feraud@orange.com</a>, Orange Labs Lannion<o:p></o:p></p>
<p class=3D"MsoNormal">Encadrement acad=E9mique&nbsp;: Odalric Maillard, <a=
 href=3D"mailto:odalric.maillard@inria.fr">
odalric.maillard@inria.fr</a>, LRI Orsay<o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoNormal"><b>Objectif de la th=E8se <o:p></o:p></b></p>
<p class=3D"MsoNormal"><b><o:p>&nbsp;</o:p></b></p>
<p class=3D"MsoNormal">L&#8217;objectif de cette th=E8se est de d=E9veloppe=
r et d&#8217;analyser des algorithmes de bandit contextuel =E0 m=E9moire po=
ur l&#8217;optimisation du choix d&#8217;actions dans un environnement dyna=
mique.<o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoNormal">L&#8217;automatisation et la prolif=E9ration des alg=
orithmes de d=E9cisions dans les services et r=E9seaux de t=E9l=E9communica=
tion n=E9cessitent de contr=F4ler le co=FBt potentiel des mauvaises d=E9cis=
ions. C&#8217;est pourquoi il est n=E9cessaire d&#8217;utiliser des algorit=
hmes
 disposants de fortes garanties th=E9oriques. <o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoNormal">Les algorithmes =E9tudi=E9s lors de la th=E8se seron=
t test=E9s en simulation sur des cas d&#8217;usages d&#8217;optimisation ma=
rketing, de routage et d&#8217;allocation de ressources dans un r=E9seau de=
 t=E9l=E9communication. A l&#8217;issue de la th=E8se, l&#8217;objectif est=
 d&#8217;int=E9grer
 les algorithmes pour une utilisation op=E9rationnelle.<o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoNormal"><b>Etat de l&#8217;art<o:p></o:p></b></p>
<p class=3D"MsoNormal"><b><o:p>&nbsp;</o:p></b></p>
<p class=3D"MsoNormal">En interagissant avec des flux d&#8217;=E9v=E8nement=
s, des algorithmes de machine learning optimisent le choix des publicit=E9s=
, choisissent la meilleure interface homme machine, recommandent des produi=
ts, assurent un premier niveau de service apr=E8s-vente,
 s=E9lectionnent le meilleur r=E9seau sans fil pour chaque mobile, optimise=
nt l&#8217;allocation de ressources dans un r=E9seau de t=E9l=E9communicati=
on... Dans la plupart de ces applications, pour choisir l&#8217;action un c=
ontexte est observable (par exemple le profil du client),
 le retour de l&#8217;environnement est partiel (i.e. seule la r=E9compense=
 de l&#8217;action choisie est connue), et la s=E9quence de r=E9compenses e=
st g=E9n=E9r=E9e par un syst=E8me dynamique inconnu (des changements de la =
distribution des r=E9compenses et des contextes peuvent intervenir).<o:p></=
o:p></p>
<p class=3D"MsoNormal">Le probl=E8me des bandits manchots consiste =E0 opti=
miser s=E9quentiellement le choix d&#8217;une action lorsque l&#8217;enviro=
nnement ne fournit qu&#8217;un retour partiel. Les algorithmes sont =E9valu=
=E9s en termes de regret entre les r=E9compenses obtenues par les
 actions choisies et celles qui auraient =E9t=E9 obtenues en choisissant la=
 meilleure action. Des solutions optimales en termes de regret et efficaces=
 en termes de temps de traitement ont =E9t=E9 propos=E9es [1].<o:p></o:p></=
p>
<p class=3D"MsoNormal">Pour prendre ses d=E9cisions l&#8217;algorithme disp=
ose souvent d&#8217;un contexte. Dans le cas de l&#8217;optimisation de cam=
pagnes marketing, le contexte est le profil d&#8217;un client et l&#8217;ac=
tion est le choix de la campagne marketing: quelle campagne marketing
 pour quel client ? Le probl=E8me des bandits contextuels consiste =E0 cons=
truire un mod=E8le s=E9quentiellement pour explorer et exploiter un ensembl=
e d&#8217;actions en fonction des contextes observ=E9s. Le probl=E8me du ba=
ndit contextuel peut =EAtre r=E9duit =E0 une s=E9rie de probl=E8mes
 de bandits manchots en utilisant des mod=E8les hi=E9rarchiques tels que le=
s arbres de d=E9cisions ou les for=EAts al=E9atoires [2].
<o:p></o:p></p>
<p class=3D"MsoNormal">Dans la plupart des applications, les s=E9quences de=
 contextes et de r=E9compenses ne peuvent pas =EAtre consid=E9r=E9es comme =
=E9tant identiquement et ind=E9pendamment distribu=E9es. Par exemple, lors =
de l&#8217;optimisation de campagnes marketing, une nouvelle
 offre chez un concurrent peut changer significativement la r=E9action de t=
ous les clients face =E0 une ou plusieurs des campagnes marketing. Pour g=
=E9rer ces changements, une premi=E8re approche consiste =E0 faire l&#8217;=
hypoth=E8se que les s=E9quences de contextes et de r=E9compenses
 ont =E9t=E9 g=E9n=E9r=E9es =E0 l&#8217;avance par un adversaire. Des solut=
ions optimales pour le probl=E8me des bandits adverses ont =E9t=E9 propos=
=E9es [3]. N=E9anmoins, ces solutions sont peu efficaces en pratique car tr=
op conservatrices.
<o:p></o:p></p>
<p class=3D"MsoNormal">Les changements de l&#8217;environnement sont parfoi=
s bien connus: le comportement d&#8217;achat pendant la p=E9riode des f=EAt=
es est diff=E9rent de celui du reste de l&#8217;ann=E9e. L&#8217;id=E9e nat=
urelle est alors d&#8217;utiliser un Processus de D=E9cision Markovien pour
 les mod=E9liser. Dans le cas g=E9n=E9ral, la structure des =E9tats n&#8217=
;est pas connue. Les Processus de D=E9cision Markovien Partiellement Observ=
ables peuvent mod=E9liser l&#8217;incertitude sur le mod=E8le =E0 =E9tats. =
N=E9anmoins, les solutions optimales sont incalculables [4]. Dans
 le cadre de l&#8217;Apprentissage par Renforcement de nombreuses solutions=
 approch=E9es ont =E9t=E9 propos=E9es [5] se basant sur le Q-learning, sur =
les Mod=E8les de Markov Cach=E9s, les R=E9seaux de Neurones R=E9currents...
<o:p></o:p></p>
<p class=3D"MsoNormal">R=E9cemment, pour d=E9passer les limites des R=E9sea=
ux de Neurones R=E9currents et notamment la difficult=E9 d&#8217;apprendre =
une m=E9moire =E0 long terme, plusieurs auteurs ont propos=E9 d&#8217;utili=
ser une m=E9moire explicite [6]. Cette id=E9e nous parait prometteuse
 pour =E9tudier les bandits contextuels dans un environnement dynamique:<o:=
p></o:p></p>
<p class=3D"MsoNormal">Pour prendre une d=E9cision, le joueur se base sur s=
a m=E9moire (i.e. un =E9tat du syst=E8me, ou un =E9v=E8nement pass=E9) et s=
ur l&#8217;observation du contexte. La r=E9compense permet de mettre =E0 jo=
ur la politique du joueur et sa m=E9moire.<o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoNormal"><b>Approche m=E9thodologique-planning <o:p></o:p></b=
></p>
<p class=3D"MsoNormal"><b><o:p>&nbsp;</o:p></b></p>
<p class=3D"MsoNormal">Pour r=E9aliser ce travail de recherche, le doctoran=
t devra maitriser les algorithmes et mod=E8les d&#8217;apprentissage par re=
nforcement, et notamment les bandits manchots, les bandits contextuels, les=
 Processus de D=E9cision Markoviens. Un travail
 approfondi de bibliographie est crucial. Quelques tests devront =EAtre men=
=E9s en parall=E8le sur les principaux algorithmes propos=E9es par l&#8217;=
=E9tat de l&#8217;art afin de bien comprendre leurs avantages et inconv=E9n=
ients. Le doctorant pourra ensuite aborder la principale
 difficult=E9 de l&#8217;analyse des bandits =E0 m=E9moire =E0 savoir le fa=
it que la classe des Processus de D=E9cision Markovien Partiellement Observ=
ables fait partie de la classe des probl=E8mes PSPACE-Hard, et qu&#8217;il =
n&#8217;existe donc pas d&#8217;algorithme efficace pour r=E9soudre ce
 probl=E8me dans le cas g=E9n=E9ral. Le doctorant devra =E9tablir des hypot=
h=E8ses raisonnables sur le mod=E8le de repr=E9sentation des =E9tats du sys=
t=E8me, afin d&#8217;=E9laborer des algorithmes efficaces et disposant de f=
orte garantie th=E9orique. Il pourra notamment s&#8217;inspirer des
 travaux de [7] et [8]. <o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoNormal">La deuxi=E8me difficult=E9, que devra aborder le doc=
torant, est li=E9e au fait que l&#8217;=E9valuation empirique d&#8217;un al=
gorithme interagissant avec un environnement, qui ne fournit que la r=E9com=
pense des actions choisies, est un probl=E8me en soi. D=E9ployer
 un algorithme dans un environnement op=E9rationnel =E0 des fins d&#8217;=
=E9valuation n&#8217;=E9tant pas une option possible, il sera n=E9cessaire =
de recourir =E0 des simulations. Dans un premier temps elles seront produit=
es avec des donn=E9es synth=E9tiques ou =E0 partir de jeux de donn=E9es
 de r=E9f=E9rence afin de se comparer avec l&#8217;=E9tat de l&#8217;art. C=
es tests seront ensuite compl=E9t=E9s avec des simulations sur des donn=E9e=
s de logs provenant du SI d&#8217;Orange afin de s&#8217;approcher au mieux=
 des cas d&#8217;usages r=E9els.<o:p></o:p></p>
<p class=3D"MsoNormal"><o:p>&nbsp;</o:p></p>
<p class=3D"MsoCommentText"><b><span lang=3D"EN-US" style=3D"font-size:11.0=
pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;">R=E9f=E9rences:<=
o:p></o:p></span></b></p>
<p class=3D"MsoNormal" style=3D"text-align:justify"><span lang=3D"EN-US" st=
yle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN">[1] Auer, P., Cesa Bianchi, N., Fischer=
, P.: Finite-time Analysis of the Multiarmed Bandit Problem, Machine Learni=
ng,47, 235-256, 2002.<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN">[2] F=E9raud, R., Allesiardo, R., Urvoy=
, T., Cl=E9rot, F.: Random Forest for the Contextual Bandit Problem, AISTAT=
S 2016.<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-align:justify"><span lang=3D"EN-US" st=
yle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN">[3] Auer, P., Cesa-Bianchi, N., Freund,=
 Y., Schapire, R. E.: The nonstochastic multiarmed bandit problem, SIAM J. =
COMPUT., 32 48-77, 2002.<o:p></o:p></span></p>
<p class=3D"MsoNormal"><span lang=3D"EN-US" style=3D"mso-fareast-language:F=
R"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN">[4] Papadimitriou, C. H., Tsitsiklis, J=
., N.&nbsp;: The Complexity of Markov Decision Processes, Mathematics of Op=
erations Research, 1987.<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal"><span lang=3D"EN-US">[5] Aberdeen, D.: A (revised) s=
urvey of approximate methods for solving partially observable Markov decisi=
on processes,Technical report, Research School of Information Science and E=
ngineering, Australia National University,
 2003.</span><span lang=3D"EN-US" style=3D"mso-fareast-language:FR"><o:p></=
o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN">[6] Graves, A., Wayne, G., Danihelka, I=
.: Neural Turing Machines,&nbsp;http://arxiv.org/abs/1410.5401, 2015.<o:p><=
/o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN">[7] Maillard, O. A., Munos, R., Ryabko,=
 D.: Selecting the State-Representation in Reinforcement Learning, NIPS, 20=
12.<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US" s=
tyle=3D"mso-fareast-language:ZH-CN"><o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-autospace:none"><span lang=3D"EN-US">[=
8] Hamilton, W., Milani Fard, M., Pineau, J.: Efficient Learning and Planni=
ng with Compressed Predictive States, JMLR, 2014.</span><span lang=3D"EN-US=
" style=3D"mso-fareast-language:FR"><o:p></o:p></span></p>
<p class=3D"MsoNormal"><span lang=3D"EN-US"><o:p>&nbsp;</o:p></span></p>
</div>
<PRE>______________________________________________________________________=
___________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confiden=
tielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu=
 ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages el=
ectroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou =
falsifie. Merci.

This message and its attachments may contain confidential or privileged inf=
ormation that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and dele=
te this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been =
modified, changed or falsified.
Thank you.
</PRE></body>
</html>

--_000_CFC3D1D3F172164888E30D094F2A10920FB94FDFOPEXCLILM23corp_--

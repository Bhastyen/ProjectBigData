{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Begin 2020-01-23 00:48:47.805852\n"
     ]
    }
   ],
   "source": [
    "import time, re, pdfx, unidecode\n",
    "import datetime\n",
    "\n",
    "# hyper-parameters\n",
    "ref_author = True\n",
    "min_number_of_common_works = 6\n",
    "number_of_top = 10\n",
    "\n",
    "# laod files into RDD\n",
    "csv_articles = sc.textFile(\"data/export_articles_EGC_2004_2018.csv\")\n",
    "dico_fr = sc.textFile(\"dictionnary/lexique-collecte.txt\")\n",
    "dico_en = sc.textFile(\"dictionnary/words.txt\")\n",
    "stop_word_fr = sc.textFile(\"dictionnary/stop-word-french.txt\")\n",
    "stop_word_en = sc.textFile(\"dictionnary/stop-word-en.txt\")\n",
    "\n",
    "# print the date of beginning\n",
    "date1 = datetime.datetime.now()\n",
    "print(\"Date Begin\", date1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"revue des nouvelles technologies de l'information\", 'egc', '2018', \"#idéo2017 : une plateforme citoyenne dédiée à l'analyse des tweets lors des événements politiques\", \"cette plateforme a pour objectif de permettre aux citoyens d'analyserpar eux-mêmes les tweets politiques lors d'événements spécifiques en france.pour le cas de l'élection présidentielle de 2017, #idéo2017 analysait en quasitemps réel les messages des candidats, et fournissait leurs principales caractéristiques,l'usage du lexique politique et des comparaisons entre les candidats.\", 'claudia marinica, julien longhi, nader hassine, abdulhafiz alkhouli, boris borzic', 'http://editions-rnti.fr/render_pdf.php?p1&p=1002425')]\n",
      "['flexion', 'de', 'de', 'et', 'à', 'des', 'du', 'que', 'la', 'la']\n",
      "['2', '1080', '&c', '10-point', '10th', '11-point', '12-point', '16-point', '18-point', '1st', '2,4,5-t', '2,4-d', '20-point', '2d', '2nd', '30-30', '3d', '3-d', '3m', '3rd']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'alors', 'à', 'au', 'aucuns']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "# split data for csv file\n",
    "def split_line_articles(line):   # create list of article as a tuple\n",
    "    items = line.split(\"\\t\")\n",
    "    return (items[0].lower(), items[1].lower(), items[2].lower(), items[3].lower(), items[4].lower(), items[5].lower(), items[6].lower())\n",
    "\n",
    "def split_line_dico_fr(line):\n",
    "    items = line.split(\"\\t\")\n",
    "    return items[2].lower()\n",
    "\n",
    "csv_articles = csv_articles.map(split_line_articles)\n",
    "dico_fr = dico_fr.map(split_line_dico_fr)\n",
    "dico_en = dico_en.map(lambda w : w.lower())\n",
    "stop_word_fr = stop_word_fr.map(lambda w : w.lower())\n",
    "stop_word_en = stop_word_en.map(lambda w : w.lower())\n",
    "\n",
    "print(csv_articles.collect()[1:2])\n",
    "print(dico_fr.collect()[:10])\n",
    "print(dico_en.collect()[:20])\n",
    "print(stop_word_fr.collect()[:30])\n",
    "print(stop_word_en.collect()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the name of authors in a string\n",
    "def find_ref(refs):\n",
    "    result = []\n",
    "    groups = re.search(r'[A-Z][a-z\\-]*\\s*,\\s*[A-Z]|(,\\s*|(et|and)\\s*)[A-Z]\\s*[A-Z]\\s*[A-Za-zç\\-\\']*', refs)\n",
    "\n",
    "    if groups:\n",
    "        value = groups.group()\n",
    "        # process the beginning of name\n",
    "        if value.startswith(\",et\"):\n",
    "            value = value[3:]\n",
    "        elif value.startswith(\", et\"):\n",
    "            value = value[4:]\n",
    "        elif value.startswith(\" et\"):\n",
    "            value = value[3:]\n",
    "        elif value.startswith(\"et\"):\n",
    "            value = value[2:]\n",
    "        elif value.startswith(\",and\"):\n",
    "            value = value[4:]\n",
    "        elif value.startswith(\", and\"):\n",
    "            value = value[5:]\n",
    "        elif value.startswith(\" and\"):\n",
    "            value = value[4:]\n",
    "        elif value.startswith(\"and\"):\n",
    "            value = value[3:]\n",
    "        elif value.startswith(\",\"):\n",
    "            value = value[1:]\n",
    "\n",
    "        if value.startswith(\" \"):\n",
    "            value = value[1:]\n",
    "        elif value.startswith(\"  \"):\n",
    "            value = value[2:]\n",
    "\n",
    "        # change the form to get always the same format of name\n",
    "        if re.match(r'[A-Za-z\\-]*\\s*,\\s*[A-Z]', value):\n",
    "            value = value[value.find(\",\") + 1:] + \" \" + value[:value.find(\",\")]\n",
    "\n",
    "        # process the spaces at the beginning of name\n",
    "        if value.startswith(\" \"):\n",
    "            value = value[1:]\n",
    "        elif value.startswith(\"  \"):\n",
    "            value = value[2:]\n",
    "\n",
    "        # process the spaces at the end of name\n",
    "        if value.endswith(\" \"):\n",
    "            value = value[:-1]\n",
    "        elif value.endswith(\"  \"):\n",
    "            value = value[:-2]\n",
    "\n",
    "        # convert accent characters\n",
    "        #value = unidecode.unidecode(value)\n",
    "\n",
    "        # last verification to remove common words\n",
    "        if re.match(r'[A-Z](\\s|-)', value):\n",
    "            result.append(value)\n",
    "\n",
    "        return result + find_ref(refs[groups.span()[1]:])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all text of pdf\n",
    "def load_text_pdf(url, id):\n",
    "    refs = \"\"\n",
    "    text = \"\"\n",
    "    marker = -1\n",
    "    reference_find = False\n",
    "    \n",
    "    try:\n",
    "        pdf = pdfx.PDFx(\"data/pdf/\" + id + \".pdf\")\n",
    "        text = pdf.get_text()\n",
    "    except: print(\"Failed\", id)\n",
    "    \n",
    "    if text != \"\":\n",
    "        pages = text.split(\"\\n\\n\")\n",
    "        for p in range(len(pages) - 1, 0, -1):   # we search on each page p from the end ...\n",
    "            for s in pages[p].split(\".\"):   # ... on each sentence s ...\n",
    "                if \"References\" in s or \"Références\" in s:  # ... the page with references part\n",
    "                    marker = p\n",
    "                    break\n",
    "            if marker != -1:\n",
    "                break\n",
    "\n",
    "        if marker != -1:\n",
    "            for p in range(marker, len(pages)):   # we process only the pages with references\n",
    "                for s in pages[p].split(\".\"):\n",
    "                    if reference_find or \"References\" in s or \"Références\" in s:  # ... the page with references part\n",
    "                        refs += s.replace(\"\\n\", \"\") + \" \"  # we keep all characters after the references have benn found\n",
    "                        reference_find = True\n",
    "    return refs\n",
    "\n",
    "def get_text_pdf(art):\n",
    "    url = art[6]\n",
    "    id = url[url.find(\"&\") + 3:]\n",
    "    \n",
    "    begin = time.time()\n",
    "    contentPdf = load_text_pdf(url, id)\n",
    "    \n",
    "    return (id, contentPdf, (time.time() - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the references part of the article\n",
    "def parse_references_aux(text):\n",
    "    refs = \"\"\n",
    "    marker = -1\n",
    "    reference_find = False\n",
    "    \n",
    "    pages = text.split(\"\\n\\n\")\n",
    "    for p in range(len(pages) - 1, 0, -1):   # we search on each page p from the end ...\n",
    "        for s in pages[p].split(\".\"):   # ... on each sentence s ...\n",
    "            if \"References\" in s or \"Références\" in s:  # ... the page with references part\n",
    "                marker = p\n",
    "                break\n",
    "        if marker != -1:\n",
    "            break\n",
    "\n",
    "    if marker != -1:\n",
    "        for p in range(marker, len(pages)):   # we process only the pages with references\n",
    "            for s in pages[p].split(\".\"):\n",
    "                if reference_find or \"References\" in s or \"Références\" in s:  # ... the page with references part\n",
    "                    refs += s.replace(\"\\n\", \"\") + \" \"  # we keep all characters after the references have benn found\n",
    "                    reference_find = True\n",
    "    \n",
    "    return refs\n",
    "    \n",
    "def parse_references(art):\n",
    "    \n",
    "    begin = time.time()\n",
    "    contentPdf = parse_references_aux(art[1])\n",
    "    \n",
    "    return (art[0], contentPdf, art[2] + (time.time() - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of authors for each quotation\n",
    "def get_names(refs):\n",
    "    content_pdf = []\n",
    "    \n",
    "    if refs != \"\":  # if we have references in this article\n",
    "        for r in re.split(r'\\([1-2][0-9][0-9][0-9]\\)', refs):  # we split per date\n",
    "            content_pdf.append(find_ref(unidecode.unidecode(r)))   # we search the name of authors\n",
    "\n",
    "    return content_pdf\n",
    "    \n",
    "def get_names_of_references(art):\n",
    "    \n",
    "    begin = time.time()\n",
    "    contentPdf = get_names(art[1])\n",
    "    \n",
    "    return (art[0], contentPdf, art[2] + (time.time() - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles :  1546 0.2553408145904541\n"
     ]
    }
   ],
   "source": [
    "csv_articles = csv_articles.map(get_text_pdf)\n",
    "articles = csv_articles.collect()\n",
    "#print(\"GET PDF TEXT\")\n",
    "print(\"Articles : \", len(articles[1][1]), articles[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_articles = csv_articles.map(parse_references)\n",
    "#articles = csv_articles.collect()\n",
    "#print(\"GET PDF REFS\")\n",
    "#for art in articles[1:2]:\n",
    "#    print(\"Articles : \", art[1], art[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET AUTHORS\n",
      "Articles :  [['C Alduy'], ['M Conover', 'J Ratkiewicz', 'M Francisco', 'B Goncalves', 'A Flammini', 'F Menczer'], ['D Avello', 'P T Metaxas', 'E Mustafaraj'], ['R Adamic', 'S Counts', 'J Longhi'], ['J Longhi', 'C Marinica', 'N Haddioui'], ['C Negationetreference', 'A Mercier'], ['S Roginsky', 'B D Cock'], ['A Tumasjan', 'T O Sprenger', 'P G Sandner', 'I M Welpe'], []] 0.2373671531677246\n",
      "Articles :  [['J Bennett', 'S Lanning'], ['H Bock'], ['M Boulle'], ['G Guyon', 'G Cawley', 'A Saffari', 'C Castillo', 'K Chellapilla', 'L Denoyer'], ['C Beijing', 'I Dhillon', 'S S', 'D S Modha'], ['K Mining', 'N York', 'U Y', 'G Govaert', 'M Nadif'], ['G Govaert', 'M Nadif'], ['R Guigoures', 'D Gay', 'M Boulle', 'F Clerot', 'F Rossi'], ['J Hartigan'], ['H Li', 'N Abe'], ['P Stroudsburg', 'U A', 'I Mechelen', 'H V', 'P D Boeck'], ['T Mitchell'], ['N York', 'U Y', 'I Hill', 'S Papadimitriou', 'J Sun'], []] 0.7898545265197754\n",
      "Articles :  [['R A', 'A J', 'C J Tauro'], ['M Ghossein', 'T Abdessalem'], ['A Chaney', 'D J', 'T Eliassi-Rad'], ['P Gopalan', 'J M Hofman', 'D M Blei'], ['J Griesner', 'T Abdessalem', 'H Naacke'], ['Y Hu', 'Y Koren', 'C Volinsky'], ['D Lee', 'H S Seung'], ['D Lian', 'C Zhao', 'X Xie', 'G Sun', 'E Chen', 'Y Rui'], ['B Liu', 'H Xiong'], ['H Ma', 'T C Zhou', 'M R Lyu', 'I King'], ['X Ning', 'G Karypis'], ['N York', 'U Y', 'S Rendle', 'C Freudenthaler', 'Z Gantner', 'L Schmidt-Thieme'], ['V Arlington', 'R Salakhutdinov', 'A Mnih'], ['I Associates', 'B Thomee', 'D A Shamma', 'G Friedland', 'B Elizalde', 'K Ni', 'D Poland', 'D Borth'], ['M Ye', 'P Yin'], ['W Zhang', 'J Wang'], []] 0.8028528690338135\n",
      "Articles :  [] 0.17253708839416504\n",
      "Articles :  [['C Aone', 'M E Okurowski', 'J Gorlinsky'], ['P Bojanowski', 'E Grave', 'A Joulin', 'T Mikolov'], ['J Conroy', \"D P O'leary\"], ['C Loupy', 'M Guegan', 'C Ayache', 'S Seng'], ['D Gildea', 'D Jurafsky'], ['A Khan', 'N Salim', 'Y J Kumar'], ['L Svore', 'C J Burges'], ['D Laurent', 'B Chardon', 'S Negre', 'C Pradel', 'P Seguela'], ['C Lin'], ['E Lloret', 'M Palomar'], ['I Mani'], ['V Automaticsummarization', 'A Nenkova', 'L Vanderwende'], ['H Saggion', 'T Poibeau'], ['N Salim', 'L Suanmali', 'M Binwahlan'], [], []] 0.3061819076538086\n",
      "Articles :  [] 0.18849635124206543\n",
      "Articles :  [['A Abboute', 'Y Boudjeriou', 'G Entringer', 'J Aze', 'S Bringay', 'P Poncelet'], ['F Aveline', 'C Baudelot', 'M Beverraggi', 'S Lahlou'], ['M Barrigon', 'S L', 'J Berrouiguet', 'C Carballo', 'P Gimenez', 'B Navarro', 'D Pfang', 'P Gomez', 'F Courtet', 'J Aroca'], ['R Beck', 'J W', 'A T Beck'], ['S Berrouiguet', 'M L Barrigon', 'S A Brandt', 'G C Nitzburg', 'S Ovejero', 'R Alvarez-Garcia', 'J Carballo', 'M Walter', 'R Billot', 'P Lenca'], ['S Berrouiguet', 'M L Barrigon', 'S A Brandt', 'S Ovejero-Garcia', 'R Alvarez-Garcia', 'J J Carballo', 'P Lenca', 'P Courtet', 'E Baca-Garcia', 'S Berrouiguet', 'R Billot', 'P Lenca', 'E Baca-Garcia', 'B Gourvennec', 'M Simonnet', 'P Tan-guy', 'S Applications', 'S Berrouiguet', 'P Courtet', 'M Perez-Rodriguez', 'M Oquendo', 'E Baca-Garcia'], ['L Breiman', 'J H Friedman', 'R A Olshen', 'C Stone'], ['M Chan', 'H K', 'N Bhatti', 'S Meader', 'J Stockton', 'R Evans', 'N Connor', 'T Kendall'], ['C Choo', 'J Diederich', 'I S adnRogerHo'], ['P Combes', 'S Combes', 'M Monziols'], ['F Montpellier', 'D Gomez', 'H Blasco-Fontecilla', 'A A Alegria', 'T Legido-Gil', 'A Artes-Rodriguez', 'E Baca-Garcia'], ['E Durkheim'], ['P Alcan', 'C Faure'], ['Y Finkelstein', 'E M Macdonald', 'S Hollands', 'M L Sivilotti', 'J R Hutson', 'M M Mamdani', 'G Koren', 'D N Juurlink'], ['C Glenn', 'M K Nock'], ['C Karmakar', 'W Luo', 'T Tran', 'M Berk', 'S Venkatesh'], ['R Kessler', 'C Warner', 'C Ivany'], ['Y Lecrubier', 'D V Sheehan', 'E Weiller', 'P Amorim', 'I Bonora', 'K H Sheehan', 'J Janavs', 'G C Dunbar'], ['J Castroman', 'H Blasco-Fontecilla', 'P Courtet', 'E Baca-Garcia', 'M A Oquendo'], ['J Castroman', 'E Nogue', 'S Guillaume', 'M C Picot', 'P Courtet'], ['A Loranger', 'N Sartorius', 'A Andreoli', 'P Berger', 'P Buchheim', 'S Channabasavanna', 'B Coid', 'A Dahl', 'R Diekstra', 'B Ferguson', 'L Jacobsberg', 'W Mombour', 'C Pull', 'Y Ono', 'D Regier'], ['D Alcohol', 'M Lytle', 'V Silenzio', 'E Caine'], ['C Maigrot', 'S Bringay', 'J Aze'], ['V Connaissances', 'R I', 'I Montaigne'], ['M Moreno', 'L A', 'K Jelenchick', 'E Egan', 'H Cox', 'K Young', 'T Becker'], [], ['J Patton', 'M H', 'E S Barratt'], ['G Rakesh'], ['J Ribeiro', 'J D', 'K Franklin', 'K Fox', 'E Bentley', 'B Kleiman', 'M K Nock'], [], ['K Smith'], ['T Tran', 'W Luo', 'D Phung', 'R Harvey', 'M Berk', 'R L Kennedy', 'S Venkatesh'], ['C Walsh', 'J D Ribeiro', 'J C Franklin'], ['A Weisman', 'J W Worden'], [], ['T Wolodzko', 'A Kokoszka', 'T Wolodzko', 'A Kokoszka']] 1.212756633758545\n",
      "Articles :  [['O Ismaili'], ['S Harbi', 'V J Rayward-Smith'], ['D Arthur', 'S Vassilvitskii'], ['S Basu', 'A Banerjee', 'R J Mooney'], ['A Bondu', 'M Boulle', 'D Gay'], ['F Toulouse', 'G Bouchard', 'B Triggs'], ['M Boulle', 'M Boulle', 'M Boulle'], ['E Connaissances', 'M Celebi', 'H E', 'P A Vela'], ['C Eick', 'N F', 'Z Zhao'], ['I Guyon', 'A Elisseeff'], ['M Hall', 'E Frank', 'G Holmes', 'B Pfahringer', 'P Reutemann', 'I H Witten'], ['D Hand', 'K Yu'], ['K Hornik'], ['O Ismaili', 'V A', 'A Cornuejols'], ['R Kohavi'], ['N Landwehr', 'M Hall', 'E Frank'], ['P Langley', 'S Sage'], ['S Intelligence', 'C Francisco', 'U A', 'V Lemaire', 'O AlaouiIsmaili', 'A Cornuejols'], ['V Lemaire', 'C Hue', 'O Bernier'], ['K Prize', 'M Lichman'], [], ['M Natick', 'G Milligan', 'M C Cooper'], ['C Salperwyck', 'V Lemaire'], ['R Vilalta', 'C F Eick'], []] 0.8277859687805176\n",
      "Articles :  [['S Agrawal', 'J Agrawal', 'S Kaur', 'S Sharma'], ['B Meunier', 'C Marsala', 'M Ramdani'], ['M Boutell', 'J R', 'X Luo', 'C M Brown'], ['S Destercke'], ['A Elisseeff', 'J Weston'], ['J Furnkranz', 'E Hullermeier', 'E LozaMencia', 'K Brinker'], ['E Gibaja', 'S Ventura'], ['S Godbole', 'S Sarawagi'], ['P Conference', 'A Sydney', 'C Proceedings', 'H Berlin', 'F Herrera', 'F Charte', 'A J Rivera', 'M J delJesus'], ['M Analysis', 'C Techniques', 'E Hullermeier', 'J Furnkranz', 'W Cheng', 'K Brinker'], ['M Kubat', 'R Holte', 'S Matwin'], ['H Berlin', 'K Laghmari', 'C Marsala', 'M Ramdani'], ['E Mencia', 'F Janssen'], ['P Dzeroski', 'D Panov', 'L Todorovski', 'S Bled', 'V Proceedings', 'E Mencia', 'F Janssen'], ['E Montanes', 'J R Quevedo', 'J J delCoz'], ['T Gunopulos', 'D Hofmann', 'M Vazirgiannis', 'J Quinlan'], ['C Francisco', 'U A', 'J Read'], ['Z Sun', 'Z Guo', 'M Jiang', 'X Wang', 'C Liu'], ['K Trohidis', 'G Tsoumakas', 'G Kalliris', 'I P Vlahavas'], ['E Bello', 'D Turnbull', 'G Tsoumakas', 'I Katakis'], ['C Rijsbergen'], ['X Wang', 'S An', 'H Shi', 'Q Hu'], []] 0.9704043865203857\n"
     ]
    }
   ],
   "source": [
    "csv_articles = csv_articles.map(get_names_of_references)\n",
    "articles = csv_articles.collect()\n",
    "print(\"GET AUTHORS\")\n",
    "for art in articles[1:10]:\n",
    "    print(\"Articles : \", art[1], art[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748.8845024108887\n",
      "Time to execute parsing 1748.8845024108887\n"
     ]
    }
   ],
   "source": [
    "def sum(a, b):\n",
    "    #print(a + b)\n",
    "    return a + b\n",
    "\n",
    "csv_articles = csv_articles.map(lambda art : art[2])\n",
    "t = csv_articles.reduce(sum)\n",
    "\n",
    "# calculate and print execution time to process the articles only\n",
    "print(\"Time to execute parsing\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date End 2020-01-23 01:34:26.307230\n"
     ]
    }
   ],
   "source": [
    "# print the date of end to know total execution time\n",
    "date2 = datetime.datetime.now()\n",
    "print(\"Date End\", date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
